{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a5bd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# EfficientNet-V2-M Snake Classifier for Local Machine (VS Code)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb46ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5fa64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Transforms & Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_path = './Snake_Dataset'\n",
    "split_path = './Split_Snake_Dataset'\n",
    "\n",
    "assert os.path.exists(data_path), f\"Dataset not found at {data_path}\"\n",
    "\n",
    "# Auto Split if not already split\n",
    "if not os.path.exists(split_path):\n",
    "    print(\"üõ†Ô∏è Splitting dataset...\")\n",
    "    os.makedirs(os.path.join(split_path, 'train'))\n",
    "    os.makedirs(os.path.join(split_path, 'val'))\n",
    "\n",
    "    full_dataset = ImageFolder(data_path)\n",
    "    class_names = full_dataset.classes\n",
    "    class_indices = {cls: [] for cls in class_names}\n",
    "    for path, label in full_dataset.samples:\n",
    "        cls = class_names[label]\n",
    "        class_indices[cls].append(path)\n",
    "\n",
    "    for cls in class_names:\n",
    "        os.makedirs(os.path.join(split_path, 'train', cls), exist_ok=True)\n",
    "        os.makedirs(os.path.join(split_path, 'val', cls), exist_ok=True)\n",
    "        imgs = class_indices[cls]\n",
    "        split_idx = int(len(imgs) * 0.85)\n",
    "        for img in imgs[:split_idx]:\n",
    "            shutil.copy(img, os.path.join(split_path, 'train', cls, os.path.basename(img)))\n",
    "        for img in imgs[split_idx:]:\n",
    "            shutil.copy(img, os.path.join(split_path, 'val', cls, os.path.basename(img)))\n",
    "else:\n",
    "    print(\"üìÅ Dataset already split.\")\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(split_path, 'train'), transform=transform)\n",
    "val_dataset = ImageFolder(os.path.join(split_path, 'val'), transform=transform)\n",
    "num_classes = len(train_dataset.classes)\n",
    "dataset_classes = train_dataset.classes\n",
    "\n",
    "print(f\"ü™≤ Classes: {num_classes} => {dataset_classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594be35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Model\n",
    "model = timm.create_model('tf_efficientnetv2_m.in21k_ft_in1k', pretrained=True, drop_rate=0.3, drop_path_rate=0.3)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.BatchNorm1d(model.num_features),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(model.num_features, num_classes)\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4cb5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4. Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7a84a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. Training Loop\n",
    "num_epochs = 15\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds += predicted.cpu().tolist()\n",
    "            all_labels += labels.cpu().tolist()\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    scheduler.step(epoch + val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc {train_acc:.2f}%, Val Acc {val_acc:.2f}%\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_efficientv2m.pth\")\n",
    "        print(\"‚úÖ Saved Best Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdb555",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. Evaluation Report\n",
    "print(\"\\nüìä Final Evaluation Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=dataset_classes))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset_classes, yticklabels=dataset_classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e41fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 7. Inference from User Image\n",
    "img_path = input(\"Enter path to image: \").strip()\n",
    "if os.path.exists(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = inference_transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            pred = model(img_tensor)\n",
    "            probs = torch.softmax(pred, dim=1)\n",
    "            top3_probs, top3_indices = torch.topk(probs, 3)\n",
    "\n",
    "    print(f\"\\nüß† Prediction for '{img_path}':\")\n",
    "    for i in range(3):\n",
    "        print(f\"{dataset_classes[top3_indices[0][i]]}: {top3_probs[0][i].item()*100:.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå Image path not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
